# ğŸ“Š AnÃ¡lisis Comparativo AutomÃ¡tico

## âœ… IntegraciÃ³n Completa

El anÃ¡lisis comparativo ahora se ejecuta **automÃ¡ticamente** en cada auditorÃ­a del pipeline.

## ğŸš€ CaracterÃ­sticas

### 1. **CÃ¡lculo AutomÃ¡tico de Puntajes**
Cada auditorÃ­a (target + competidores) recibe puntajes en 4 categorÃ­as:

- **Estructura** (0-100): H1, jerarquÃ­a, HTML semÃ¡ntico
- **Contenido** (0-100): Claridad, tono conversacional, FAQs
- **E-E-A-T** (0-100): Autor, fechas, transparencia, enlaces
- **Schema** (0-100): Presencia y tipos de Schema.org
- **Total** (0-100): Promedio de las 4 categorÃ­as

### 2. **Ranking AutomÃ¡tico**
Los sitios se ordenan por puntaje total, mostrando la posiciÃ³n competitiva.

### 3. **Fortalezas y Debilidades**
IdentificaciÃ³n automÃ¡tica:
- **Fortalezas**: Puntajes â‰¥ 70
- **Debilidades**: Puntajes < 50

### 4. **Reportes Generados**

#### `comparative_report.html`
- VisualizaciÃ³n interactiva con grÃ¡ficos
- Tablas comparativas
- AnÃ¡lisis de fortalezas/debilidades

#### `comparative_scores.json`
- Datos estructurados para integraciÃ³n
- Puntajes detallados
- Ranking y anÃ¡lisis

## ğŸ“ UbicaciÃ³n de Archivos

```
auditor/
â”œâ”€â”€ backend/
â”‚   â””â”€â”€ app/
â”‚       â””â”€â”€ services/
â”‚           â”œâ”€â”€ pipeline_service.py          # Pipeline con anÃ¡lisis integrado
â”‚           â””â”€â”€ comparative_report_generator.py  # Generador de HTML
â”œâ”€â”€ comparative_analysis.py                  # Script standalone
â””â”€â”€ reports/
    â”œâ”€â”€ comparative_report.html              # Reporte visual
    â””â”€â”€ comparative_scores.json              # Datos JSON
```

## ğŸ”§ Uso

### AutomÃ¡tico (Integrado en Pipeline)

```python
from app.services.pipeline_service import PipelineService

result = await PipelineService.run_complete_audit(
    url="https://example.com",
    # ... otros parÃ¡metros
)

# El resultado incluye:
# result['comparative_analysis'] = {
#     'scores': [...],
#     'ranking': [...],
#     'analysis': [...],
#     'summary': {...}
# }
```

### Manual (Script Standalone)

```bash
python comparative_analysis.py "path/to/final_llm_context.json"
```

## ğŸ“Š Ejemplo de Salida

### Consola
```
====================================================================================================
RANKING GENERAL
====================================================================================================
1. https://zencoder.ai/es/blog/best-ai-for-coding - 58.9/100
2. https://www.skillnest.com/blog/... - 50.3/100
3. https://cloud.google.com/use-cases/... - 42.9/100
4. SITE-WIDE AGGREGATE: https://www.codegpt.co/ - 37.0/100
====================================================================================================
```

### JSON
```json
{
  "scores": [
    {
      "url": "https://www.codegpt.co/",
      "scores": {
        "structure": 76.7,
        "content": 71.5,
        "eeat": 0.0,
        "schema": 0,
        "total": 37.0
      }
    }
  ],
  "summary": {
    "target_position": 4,
    "total_competitors": 3,
    "target_score": 37.0,
    "best_competitor_score": 58.9
  }
}
```

## ğŸ¯ Algoritmo de Scoring

### Estructura (0-100)
```python
score = 0
score += 25 if h1_check == 'pass' else 0
score += 25 if no_hierarchy_issues else 0
score += semantic_html_percent * 0.5
```

### Contenido (0-100)
```python
score = 0
score += max(0, 100 - long_paragraphs * 5)
score += conversational_tone * 10
score += 25 if has_faqs else 0
score += 25 if inverted_pyramid else 0
score = score / 2
```

### E-E-A-T (0-100)
```python
score = 0
score += 25 if has_author else 0
score += min(25, external_links * 0.5)
score += 25 if has_dates else 0
score += 25 * (about + contact + privacy) / 3
```

### Schema (0-100)
```python
score = 0
score += 50 if schema_present else 0
score += min(50, schema_types_count * 25)
```

## ğŸ”„ Flujo de EjecuciÃ³n

```mermaid
graph TD
    A[AuditorÃ­a Target] --> B[AuditorÃ­a Competidores]
    B --> C[Calcular Scores]
    C --> D[Generar Ranking]
    D --> E[Identificar Fortalezas/Debilidades]
    E --> F[Generar HTML]
    E --> G[Guardar JSON]
    F --> H[comparative_report.html]
    G --> I[comparative_scores.json]
```

## ğŸ“ˆ Beneficios

1. **AutomatizaciÃ³n**: Sin intervenciÃ³n manual
2. **Consistencia**: Mismo algoritmo para todos
3. **VisualizaciÃ³n**: GrÃ¡ficos interactivos
4. **Accionable**: Identifica prioridades claras
5. **Integrable**: JSON para dashboards/APIs

## ğŸ› ï¸ PersonalizaciÃ³n

### Ajustar Pesos de Scoring

Edita `pipeline_service.py`:

```python
@staticmethod
def calculate_scores(audit_data: Dict[str, Any]) -> Dict[str, float]:
    # Modificar pesos aquÃ­
    structure_score += 30 if h1_pass else 0  # Cambiar de 25 a 30
    # ...
```

### Agregar Nuevas MÃ©tricas

```python
# En calculate_scores()
scores['performance'] = calculate_performance_score(audit_data)
scores['total'] = sum(scores.values()) / len(scores)
```

## ğŸ› Troubleshooting

### Error: "No comparative_analysis in result"
- Verifica que `competitor_audits` no estÃ© vacÃ­o
- Revisa logs para errores en `generate_comparative_analysis`

### Reportes no se generan
- Verifica permisos de escritura en `reports/`
- Revisa que `comparative_report_generator.py` estÃ© importable

### Puntajes inesperados
- Revisa datos de entrada en `target_audit`
- Verifica que campos requeridos existan

## ğŸ“ Notas

- Los reportes se sobrescriben en cada ejecuciÃ³n
- Para mÃºltiples auditorÃ­as, renombra archivos manualmente
- El anÃ¡lisis requiere al menos 1 competidor para comparaciÃ³n

## ğŸš€ PrÃ³ximas Mejoras

- [ ] Guardar histÃ³rico de comparaciones
- [ ] GrÃ¡ficos de tendencias temporales
- [ ] Exportar a PDF
- [ ] Dashboard interactivo
- [ ] Alertas de cambios significativos
